- Namespace 技术实际上修改了应用进程看待整个计算机“视图”，即它的“视线”被操作系统做了限制，只能“看到”某些指定的内容。
- 使用虚拟化技术作为应用沙盒，就必须要由 Hypervisor 来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的
  Guest OS 才能执行用户的应用进程。这就不可避免地带来了额外的资源消耗和占用。用户应用运行在虚拟机里面，它对宿主机操作系统的调用
  就不可避免地要经过虚拟化软件的拦截和处理，这本身又是一层性能损耗，尤其对计算资源、网络和磁盘 I/O 的损耗非常大。
- “敏捷”和“高性能”是容器相较于虚拟机最大的优势，也是它能够在 PaaS 这种更细粒度的资源管理平台上大行其道的重要原因。
- 基于 Linux Namespace 的隔离机制相比于虚拟化技术也有很多不足之处，其中最主要的问题就是：隔离得不彻底。


- Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。

```shell
[root@mylinux ~]# mount -t cgroup
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)

[root@mylinux ~]# ls /sys/fs/cgroup/cpu
cgroup.clone_children  cgroup.procs          cpuacct.stat   cpuacct.usage_percpu  cpu.cfs_quota_us  cpu.rt_runtime_us  cpu.stat  kubepods           release_agent  tasks
cgroup.event_control   cgroup.sane_behavior  cpuacct.usage  cpu.cfs_period_us     cpu.rt_period_us  cpu.shares         docker    notify_on_release  system.slice   user.slice
```

- cfs_period 和 cfs_quota这两个参数需要组合使用，可以用来限制进程在长度为 cfs_period 的一段时间内，只能被分配到总量为
  cfs_quota 的 CPU 时间。

```shell
[root@mylinux ~]# cd /sys/fs/cgroup/cpu
[root@mylinux cpu]# mkdir container
[root@mylinux cpu]# ls container/
cgroup.clone_children  cgroup.procs  cpuacct.usage         cpu.cfs_period_us  cpu.rt_period_us   cpu.shares  notify_on_release
cgroup.event_control   cpuacct.stat  cpuacct.usage_percpu  cpu.cfs_quota_us   cpu.rt_runtime_us  cpu.stat    tasks
[root@mylinux cpu]# while : ; do : ; done &
[1] 71890

# 再开一个终端执行top命令【可以看到PID=71890的进程打满了CPU】
top - 17:06:43 up  2:36,  4 users,  load average: 0.94, 0.52, 0.27
Tasks: 179 total,   2 running, 177 sleeping,   0 stopped,   0 zombie
%Cpu(s): 26.2 us,  2.2 sy,  0.0 ni, 71.6 id,  0.0 wa,  0.0 hi,  0.1 si,  0.0 st
KiB Mem :  7990288 total,  5758096 free,  1178528 used,  1053664 buff/cache
KiB Swap:  8257532 total,  8257532 free,        0 used.  6437312 avail Mem 

   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                  
 71890 root      20   0  115676    688    164 R 100.0  0.0   3:18.41 bash                                                                                                                     
  9341 root      20   0 1212256 460920  69848 S   2.7  5.8   5:53.90 k3s-server                                                                                                               
 17518 root      20   0  161796   6560   4768 S   0.7  0.1   0:55.43 sshd                                                                                                                     
 ....
```

- 通过修改container 目录下的文件内容来设置限制，向 container 组里的 cfs_quota 文件写入 20 ms（20000 us），把被限制的进程的
  PID 写入 container 组里的 tasks 文件，这些设置就会对该进程生效

```shell
[root@mylinux cpu]# echo 20000 > /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us
[root@mylinux cpu]# echo 71890 > /sys/fs/cgroup/cpu/container/tasks

# 再开一个终端执行top命令【可以看到PID=71890的进程只占用了20%的CPU】
top - 17:12:38 up  2:42,  4 users,  load average: 0.38, 0.60, 0.42
Tasks: 179 total,   2 running, 177 sleeping,   0 stopped,   0 zombie
%Cpu(s):  6.0 us,  2.2 sy,  0.0 ni, 91.8 id,  0.0 wa,  0.0 hi,  0.1 si,  0.0 st
KiB Mem :  7990288 total,  5759172 free,  1170912 used,  1060204 buff/cache
KiB Swap:  8257532 total,  8257532 free,        0 used.  6444732 avail Mem 

   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                  
 71890 root      20   0  115676    688    164 R  19.9  0.0   7:39.39 bash                                                                                                                     
  9341 root      20   0 1212356 460972  69852 S   3.0  5.8   6:04.40 k3s-server                                                                                                               
  8909 root      16  -4   62056   1272    640 S   0.7  0.0   0:03.58 auditd    
  ....
```

- Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。
- 如果是为Docker容器进程分配cfs_period 和 cfs_quota，那么执行：

```shell
docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash

# 这时候查看 Cgroups 文件系统下该容器对应的控制组文件内容：
[root@mylinux ~]# cat /sys/fs/cgroup/cpu/docker/81d1dc3e92d40989bc3b8141a40a92fdab804ed42c1b6e0ceb44a381c5cd5a1b/cpu.cfs_period_us 
100000
[root@mylinux ~]# cat /sys/fs/cgroup/cpu/docker/81d1dc3e92d40989bc3b8141a40a92fdab804ed42c1b6e0ceb44a381c5cd5a1b/cpu.cfs_quota_us 
20000
```

- Cgroups 对资源的限制能力也有很多不完善的地方，被提及最多的自然是 /proc 文件系统的问题。
- Linux 下的 /proc 目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息，比如
  CPU 使用情况、内存占用率等，这些文件也是 top 指令查看系统信息的主要数据来源。
- 在容器里执行 top 指令，就会发现，它显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据。【这可以通过lxcfs方案解决】

- 一个正在运行的 Docker 容器，其实就是一个启用了多个 Linux Namespace 的应用进程，而这个进程能够使用的资源量，则受 Cgroups
  配置的限制。





